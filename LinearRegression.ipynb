{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with importing necessary libraries\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to create adjusted R-Squared\n",
    "def adj_r2(x,y):\n",
    "    r2 = regression.score(x,y)\n",
    "    n = x.shape[0]\n",
    "    p = x.shape[1]\n",
    "    adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv('Admission_Prediction.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['University Rating'] = data['University Rating'].fillna(data['University Rating'].mode()[0])\n",
    "data['TOEFL Score'] = data['TOEFL Score'].fillna(data['TOEFL Score'].mean())\n",
    "data['GRE Score']  = data['GRE Score'].fillna(data['GRE Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.drop(columns = ['Serial No.'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in data:\n",
    "    if plotnumber<=16 :\n",
    "        ax = plt.subplot(4,4,plotnumber)\n",
    "        sns.distplot(data[column])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        #plt.ylabel('Salary',fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Chance of Admit']\n",
    "X =data.drop(columns = ['Chance of Admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,30), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in X:\n",
    "    if plotnumber<=15 :\n",
    "        ax = plt.subplot(5,3,plotnumber)\n",
    "        plt.scatter(X[column],y)\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        plt.ylabel('Chance of Admit',fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "variables = X_scaled\n",
    "\n",
    "# we create a new data frame which will include all the VIFs\n",
    "# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)\n",
    "# we do not include categorical values for mulitcollinearity as they do not provide much information as numerical ones do\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \n",
    "vif[\"VIF\"] = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n",
    "# Finally, I like to include names so it is easier to explore the result\n",
    "vif[\"Features\"] = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have the correlation values for all the features. As a thumb rule, a VIF value greater than 5 means a very severe multicollinearity. We don't any VIF greater than 5 , so we are good to go.\n",
    "\n",
    "Great. Let's go ahead and use linear regression and see how good it fits our data. But first. let's split our data in train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X_scaled,y,test_size = 0.25,random_state=355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "\n",
    "regression.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model to the local file system\n",
    "filename = 'finalized_model.pickle'\n",
    "pickle.dump(regression, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1='scaler_model.pickle'\n",
    "pickle.dump(scaler,open(filename1,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using the saved model\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "a=loaded_model.predict(scaler.transform([[300,110,5,5,5,10,1]]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r2(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our r2 score is 84.15% and adj r2 is 83.85% for our training et., so looks like we are not being penalized by use of any feature.\n",
    "\n",
    "Let's check how well model fits the test data.\n",
    "\n",
    "Now let's check if our model is overfitting our data using regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r2(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like our model r2 score is less on the test data.\n",
    "\n",
    "Let's see if our model is overfitting our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regularization\n",
    "# LassoCV will return best alpha and coefficients after performing 10 cross validations\n",
    "lasscv = LassoCV(alphas = None,cv =10, max_iter = 100000, normalize = True)\n",
    "lasscv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best alpha parameter\n",
    "alpha = lasscv.alpha_\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have best parameter, let's use Lasso regression and see how well our data has fitted before\n",
    "\n",
    "lasso_reg = Lasso(alpha)\n",
    "lasso_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our r2_score for test data (75.34%) comes same as before using regularization. So, it is fair to say our OLS model did not overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ridge regression model\n",
    "# RidgeCV will return best alpha and coefficients after performing 10 cross validations. \n",
    "# We will pass an array of random numbers for ridgeCV to select best alpha from them\n",
    "\n",
    "alphas = np.random.uniform(low=0, high=10, size=(50,))\n",
    "ridgecv = RidgeCV(alphas = alphas,cv=10,normalize = True)\n",
    "ridgecv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=ridgecv.alpha_)\n",
    "ridge_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got the same r2 square using Ridge regression as well. So, it's safe to say there is no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic net\n",
    "\n",
    "elasticCV = ElasticNetCV(alphas = None, cv =10)\n",
    "\n",
    "elasticCV.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_reg = ElasticNet(alpha = elasticCV.alpha_,l1_ratio=0.5)\n",
    "elasticnet_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see by using different type of regularization, we still are getting the same r2 score. That means our OLS model has been well trained over the training data and there is no overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
